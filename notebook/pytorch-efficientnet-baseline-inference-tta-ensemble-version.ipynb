{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.02566,
     "end_time": "2020-11-23T23:46:18.651431",
     "exception": false,
     "start_time": "2020-11-23T23:46:18.625771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "package_path = '../input/pytorch-image-models/pytorch-image-models-master' #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
    "import sys; sys.path.append(package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 3.237713,
     "end_time": "2020-11-23T23:46:21.905378",
     "exception": false,
     "start_time": "2020-11-23T23:46:18.667665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "import timm #from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 0.062148,
     "end_time": "2020-11-23T23:46:22.026368",
     "exception": false,
     "start_time": "2020-11-23T23:46:21.964220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.035029,
     "end_time": "2020-11-23T23:46:22.160746",
     "exception": false,
     "start_time": "2020-11-23T23:46:22.125717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017744,
     "end_time": "2020-11-23T23:46:22.196733",
     "exception": false,
     "start_time": "2020-11-23T23:46:22.178989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.651041,
     "end_time": "2020-11-23T23:46:22.865996",
     "exception": false,
     "start_time": "2020-11-23T23:46:22.214955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.085731,
     "end_time": "2020-11-23T23:46:23.039268",
     "exception": false,
     "start_time": "2020-11-23T23:46:22.953537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.113456,
     "end_time": "2020-11-23T23:46:23.238199",
     "exception": false,
     "start_time": "2020-11-23T23:46:23.124743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, data_root, transforms=None, output_label=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']\n",
    "          \n",
    "        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n",
    "        \n",
    "        img  = get_img(path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "            \n",
    "        # do label smoothing\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063244,
     "end_time": "2020-11-23T23:46:23.369451",
     "exception": false,
     "start_time": "2020-11-23T23:46:23.306207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Train\\Validation Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 1.044006,
     "end_time": "2020-11-23T23:46:24.482278",
     "exception": false,
     "start_time": "2020-11-23T23:46:23.438272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_inference_transforms(CFG):\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024729,
     "end_time": "2020-11-23T23:46:24.532935",
     "exception": false,
     "start_time": "2020-11-23T23:46:24.508206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.035858,
     "end_time": "2020-11-23T23:46:24.593387",
     "exception": false,
     "start_time": "2020-11-23T23:46:24.557529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        ### vit\n",
    "        num_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(num_features, num_classes)\n",
    "        '''\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            #nn.Linear(num_features, hidden_size,bias=True), nn.ELU(),\n",
    "            nn.Linear(num_features, num_classes, bias=True)\n",
    "        )\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x  \n",
    "    \n",
    "    \n",
    "class CustomResNext(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        #='resnext50_32x4d',\n",
    "        n_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(n_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF_CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'img_size': 512,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 32,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 0,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'weights': [1,1,1,1,1]\n",
    "}\n",
    "\n",
    "RES_CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 719,\n",
    "    'model_arch': 'resnext50_32x4d',\n",
    "    'img_size': 512,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 32,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 0,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'weights': [1,1,1,1,1]\n",
    "}\n",
    "\n",
    "ViT_CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 719,\n",
    "    'model_arch': 'vit_base_patch16_384',\n",
    "    'img_size': 384,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 32,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 0,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'weights': [1,1,1,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023326,
     "end_time": "2020-11-23T23:46:24.640029",
     "exception": false,
     "start_time": "2020-11-23T23:46:24.616703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.038473,
     "end_time": "2020-11-23T23:46:24.701815",
     "exception": false,
     "start_time": "2020-11-23T23:46:24.663342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference fold 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:15<00:00,  1.77it/s]\n",
      "100%|██████████| 134/134 [01:15<00:00,  1.77it/s]\n",
      "100%|██████████| 134/134 [01:18<00:00,  1.71it/s]\n",
      "100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n",
      "100%|██████████| 134/134 [01:35<00:00,  1.40it/s]\n",
      "100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.74it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.76it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 validation loss = 0.31585\n",
      "fold 0 validation accuracy = 0.92757\n",
      "Inference fold 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:18<00:00,  1.70it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.76it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.75it/s]\n",
      "100%|██████████| 134/134 [01:35<00:00,  1.41it/s]\n",
      "100%|██████████| 134/134 [01:35<00:00,  1.40it/s]\n",
      "100%|██████████| 134/134 [01:17<00:00,  1.73it/s]\n",
      "100%|██████████| 134/134 [01:17<00:00,  1.74it/s]\n",
      "100%|██████████| 134/134 [01:15<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 validation loss = 0.31494\n",
      "fold 1 validation accuracy = 0.92640\n",
      "Inference fold 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:15<00:00,  1.79it/s]\n",
      "100%|██████████| 134/134 [01:14<00:00,  1.79it/s]\n",
      "100%|██████████| 134/134 [01:14<00:00,  1.79it/s]\n",
      "100%|██████████| 134/134 [01:32<00:00,  1.44it/s]\n",
      "100%|██████████| 134/134 [01:33<00:00,  1.44it/s]\n",
      "100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.74it/s]\n",
      "100%|██████████| 134/134 [01:17<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2 validation loss = 0.29904\n",
      "fold 2 validation accuracy = 0.93246\n",
      "Inference fold 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:15<00:00,  1.78it/s]\n",
      "100%|██████████| 134/134 [01:15<00:00,  1.78it/s]\n",
      "100%|██████████| 134/134 [01:17<00:00,  1.72it/s]\n",
      "100%|██████████| 134/134 [01:38<00:00,  1.36it/s]\n",
      "100%|██████████| 134/134 [01:37<00:00,  1.38it/s]\n",
      "100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n",
      "100%|██████████| 134/134 [01:18<00:00,  1.70it/s]\n",
      "100%|██████████| 134/134 [01:17<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3 validation loss = 0.31596\n",
      "fold 3 validation accuracy = 0.92709\n",
      "Inference fold 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:15<00:00,  1.78it/s]\n",
      "100%|██████████| 134/134 [01:15<00:00,  1.78it/s]\n",
      "100%|██████████| 134/134 [01:15<00:00,  1.78it/s]\n",
      "100%|██████████| 134/134 [01:35<00:00,  1.40it/s]\n",
      "100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n",
      "100%|██████████| 134/134 [01:34<00:00,  1.42it/s]\n",
      "100%|██████████| 134/134 [01:18<00:00,  1.71it/s]\n",
      "100%|██████████| 134/134 [01:16<00:00,  1.74it/s]\n",
      "100%|██████████| 134/134 [01:15<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4 validation loss = 0.31552\n",
      "fold 4 validation accuracy = 0.92054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(EF_CFG['seed'])\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=EF_CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(EF_CFG), output_label=False)\n",
    "        vi_valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(ViT_CFG), output_label=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=EF_CFG['valid_bs'],\n",
    "            num_workers=EF_CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        vi_val_loader = torch.utils.data.DataLoader(\n",
    "            vi_valid_ds, \n",
    "            batch_size=ViT_CFG['valid_bs'],\n",
    "            num_workers=ViT_CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        device = torch.device(EF_CFG['device'])\n",
    "        val_preds = []\n",
    "        \n",
    "        #for epoch in range(CFG['epochs']-3):    \n",
    "        ef_model = CassvaImgClassifier(EF_CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        ef_model.load_state_dict(torch.load('baseline_pytorch_efb4/{}_fold_{}'.format(EF_CFG['model_arch'], fold)))\n",
    "        with torch.no_grad():\n",
    "            for _ in range(EF_CFG['tta']):\n",
    "                val_preds += [1/EF_CFG['tta']/3*inference_one_epoch(ef_model, val_loader, device)]\n",
    "        del ef_model\n",
    "        \n",
    "        vi_model = CustomViT(ViT_CFG['model_arch'], train.label.nunique()).to(device)                \n",
    "        vi_model.load_state_dict(torch.load('baseline_pytorch_vit/{}_fold_{}'.format(ViT_CFG['model_arch'], fold)))     \n",
    "        with torch.no_grad():\n",
    "            for _ in range(ViT_CFG['tta']):\n",
    "                val_preds += [1/EF_CFG['tta']/3*inference_one_epoch(vi_model, vi_val_loader, device)]\n",
    "        del vi_model\n",
    "        \n",
    "        res_model = CustomResNext(RES_CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        res_model.load_state_dict(torch.load('baseline_pytorch_resnext/{}_fold_{}'.format(RES_CFG['model_arch'], fold)))    \n",
    "        with torch.no_grad():\n",
    "            for _ in range(RES_CFG['tta']):                \n",
    "                val_preds += [1/EF_CFG['tta']/3*inference_one_epoch(res_model, val_loader, device)]\n",
    "        del res_model\n",
    "        \n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n",
    "        \n",
    "        oof_ = pd.concat([valid_, pd.DataFrame(val_preds, columns=[f'soft_label_{i}' for i in range(1,6)])], axis=1)\n",
    "        oof_.to_pickle(f\"{EF_CFG['model_arch']}_oof{fold}.pkl\")\n",
    "        \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "papermill": {
     "duration": 1169.376151,
     "end_time": "2020-11-24T00:05:54.103010",
     "exception": false,
     "start_time": "2020-11-23T23:46:24.726859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 37.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.33it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 52.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    tst_preds = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "    print('device ', device)\n",
    "    \n",
    "    train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')    \n",
    "    test = pd.DataFrame()\n",
    "    test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "    \n",
    "    ## ef\n",
    "    seed_everything(EF_CFG['seed'])\n",
    "    test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(EF_CFG), output_label=False)\n",
    "    tst_loader = torch.utils.data.DataLoader(\n",
    "        test_ds, \n",
    "        batch_size=EF_CFG['valid_bs'],\n",
    "        num_workers=EF_CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    model = CassvaImgClassifier(EF_CFG['model_arch'], train.label.nunique()).to(device)\n",
    "    folds = StratifiedKFold(n_splits=EF_CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):  \n",
    "        model.load_state_dict(torch.load('baseline_pytorch_efb4/{}_fold_{}'.format(EF_CFG['model_arch'], fold), map_location=map_location))\n",
    "        with torch.no_grad():\n",
    "            for _ in range(EF_CFG['tta']):\n",
    "                tst_preds += [EF_CFG['weights'][fold]/sum(EF_CFG['weights'])/EF_CFG['tta']/3*inference_one_epoch(model, tst_loader, device)]\n",
    "    del model\n",
    "    \n",
    "    \n",
    "    ## vit\n",
    "    seed_everything(ViT_CFG['seed'])\n",
    "    test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(ViT_CFG), output_label=False)\n",
    "    tst_loader = torch.utils.data.DataLoader(\n",
    "        test_ds, \n",
    "        batch_size=ViT_CFG['valid_bs'],\n",
    "        num_workers=ViT_CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    model = CustomViT(ViT_CFG['model_arch'], train.label.nunique()).to(device)\n",
    "    folds = StratifiedKFold(n_splits=ViT_CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):  \n",
    "        model.load_state_dict(torch.load('baseline_pytorch_vit/{}_fold_{}'.format(ViT_CFG['model_arch'], fold), map_location=map_location))\n",
    "        with torch.no_grad():\n",
    "            for _ in range(ViT_CFG['tta']):\n",
    "                tst_preds += [ViT_CFG['weights'][fold]/sum(ViT_CFG['weights'])/ViT_CFG['tta']/3*inference_one_epoch(model, tst_loader, device)]\n",
    "    del model\n",
    "    \n",
    "    \n",
    "    ## resnext\n",
    "    seed_everything(RES_CFG['seed'])\n",
    "    test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(RES_CFG), output_label=False)\n",
    "    tst_loader = torch.utils.data.DataLoader(\n",
    "        test_ds, \n",
    "        batch_size=RES_CFG['valid_bs'],\n",
    "        num_workers=RES_CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    model = CustomResNext(RES_CFG['model_arch'], train.label.nunique()).to(device)\n",
    "    folds = StratifiedKFold(n_splits=RES_CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):  \n",
    "        model.load_state_dict(torch.load('baseline_pytorch_resnext/{}_fold_{}'.format(RES_CFG['model_arch'], fold), map_location=map_location))\n",
    "        with torch.no_grad():\n",
    "            for _ in range(RES_CFG['tta']):\n",
    "                tst_preds += [RES_CFG['weights'][fold]/sum(RES_CFG['weights'])/RES_CFG['tta']/3*inference_one_epoch(model, tst_loader, device)]\n",
    "                \n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "papermill": {
     "duration": 0.678515,
     "end_time": "2020-11-24T00:05:55.426721",
     "exception": false,
     "start_time": "2020-11-24T00:05:54.748206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['label'] = np.argmax(np.mean(tst_preds, axis=0), axis=1)\n",
    "test.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 1188.017422,
   "end_time": "2020-11-24T00:06:02.009298",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-23T23:46:13.991876",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
