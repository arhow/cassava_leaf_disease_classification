{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.664524,
     "end_time": "2020-11-23T13:32:47.332411",
     "exception": false,
     "start_time": "2020-11-23T13:32:46.667887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "package_paths = [\n",
    "    '../input/pytorch-image-models/pytorch-image-models-master', #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
    "    '../input/image-fmix/FMix-master'\n",
    "]\n",
    "import sys; \n",
    "\n",
    "for pth in package_paths:\n",
    "    sys.path.append(pth)\n",
    "    \n",
    "# from fmix import sample_mask, make_low_freq_image, binarise_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.173722,
     "end_time": "2020-11-23T13:32:49.521795",
     "exception": false,
     "start_time": "2020-11-23T13:32:47.348073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangz\\anaconda3\\envs\\tf_gpu23-2\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.layers import AdaptiveConcatPool2d, Flatten, Mish\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 0.057123,
     "end_time": "2020-11-23T13:32:49.643710",
     "exception": false,
     "start_time": "2020-11-23T13:32:49.586587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "train['path'] = '../input/cassava-leaf-disease-classification/train_images/' + train['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000015157.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000201771.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/100042118.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000723321.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000812911.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  \\\n",
       "0  1000015157.jpg      0   \n",
       "1  1000201771.jpg      3   \n",
       "2   100042118.jpg      1   \n",
       "3  1000723321.jpg      1   \n",
       "4  1000812911.jpg      3   \n",
       "\n",
       "                                                                       path  \n",
       "0  ../input/cassava-leaf-disease-classification/train_images/1000015157.jpg  \n",
       "1  ../input/cassava-leaf-disease-classification/train_images/1000201771.jpg  \n",
       "2   ../input/cassava-leaf-disease-classification/train_images/100042118.jpg  \n",
       "3  ../input/cassava-leaf-disease-classification/train_images/1000723321.jpg  \n",
       "4  ../input/cassava-leaf-disease-classification/train_images/1000812911.jpg  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 347.06it/s]\n"
     ]
    }
   ],
   "source": [
    "path_ = '../input/cassava-disease/train'\n",
    "data2019 = []\n",
    "for label in tqdm(os.listdir(path_)):\n",
    "    for file in os.listdir(f'{path_}/{label}'):\n",
    "        data2019.append({'image_id':file, 'label':label, 'path':f'{path_}/{label}/{file}'})\n",
    "data2019 = pd.DataFrame(data2019)\n",
    "data2019['label'] = data2019['label'].replace({'cmd':3, 'cgm':2, 'cbsd':1, 'cbb':0, 'healthy':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000015157.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000201771.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/100042118.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000723321.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000812911.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  \\\n",
       "0  1000015157.jpg      0   \n",
       "1  1000201771.jpg      3   \n",
       "2   100042118.jpg      1   \n",
       "3  1000723321.jpg      1   \n",
       "4  1000812911.jpg      3   \n",
       "\n",
       "                                                                       path  \n",
       "0  ../input/cassava-leaf-disease-classification/train_images/1000015157.jpg  \n",
       "1  ../input/cassava-leaf-disease-classification/train_images/1000201771.jpg  \n",
       "2   ../input/cassava-leaf-disease-classification/train_images/100042118.jpg  \n",
       "3  ../input/cassava-leaf-disease-classification/train_images/1000723321.jpg  \n",
       "4  ../input/cassava-leaf-disease-classification/train_images/1000812911.jpg  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, data2019], axis= 0).reset_index(drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27053/27053 [02:01<00:00, 221.84it/s]\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for idx, row in tqdm(train.iterrows(), total = train.shape[0]):\n",
    "    img = cv2.imread(row['path'])\n",
    "    lst.append({'width':img.shape[0], 'height':img.shape[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, pd.DataFrame(lst)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dataset'] = train['path'].apply(lambda x: 'cassava-leaf-disease-classification' if 'cassava-leaf-disease-classification' in x else 'cassava-disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cassava-leaf-disease-classification    21397\n",
       "cassava-disease                         5628\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[(train['width']>=500)&(train['height']>=500)]['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['width']>=500)&(train['height']>=500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    oof_df = pd.read_pickle('oof_df.pickle')\n",
    "    noisy_images = list(set(oof_df[oof_df['log_loss']>1].image_id.tolist())&set(oof_df[oof_df['euclidean']>np.quantile(oof_df['euclidean'], .95)].image_id.tolist()))\n",
    "    keep = train[~train['image_id'].isin(noisy_images)].reset_index(False)\n",
    "    removed = train[train['image_id'].isin(noisy_images)].reset_index(False)\n",
    "    print(df.shape[0], keep.shape[0], removed.shape[0])\n",
    "    return keep, removed\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    path_ = 'baseline_pytorch_efb4'\n",
    "    oof_df = pd.DataFrame()\n",
    "    for oof_pkl in [f'{path_}/{f}' for f in os.listdir(path_) if 'pkl' in f]:\n",
    "        oof_df = pd.concat([oof_df, pd.read_pickle(oof_pkl)], axis=0)\n",
    "    oof_df = oof_df.reset_index(False)\n",
    "    oof_df['confidence'] = oof_df[['label']+[f'soft_label_{i}' for i in range(1,6)]].apply(lambda x: 1-x[int(x[0]+1)], axis=1)\n",
    "    noisy_images = list(set(oof_df[oof_df['confidence']>.98].image_id.tolist()))\n",
    "    keep = df[~df['image_id'].isin(noisy_images)].reset_index(False)\n",
    "    removed = df[df['image_id'].isin(noisy_images)].reset_index(False)\n",
    "    print(df.shape[0], keep.shape[0], removed.shape[0])\n",
    "    return keep, removed\n",
    "\n",
    "# def clean_data(df):\n",
    "#     keep = df.sample(int(df.shape[0]*.99)).reset_index(False)\n",
    "#     removed = df[~df['image_id'].isin(keep.image_id.tolist())].reset_index(False)\n",
    "#     print(df.shape[0], keep.shape[0], removed.shape[0])\n",
    "#     return keep, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, removed = clean_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CassvaImgClassifier\n",
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',#'tf_efficientnet_b4_ns',\n",
    "    'img_size': 456,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 16,#16,\n",
    "    'valid_bs': 32,#32,\n",
    "    'T_0': 10,\n",
    "    'lr': 1e-4,\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay':1e-6,\n",
    "    'num_workers': 0, #4\n",
    "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'freeze_bn_epochs':5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015931,
     "end_time": "2020-11-23T13:32:49.801027",
     "exception": false,
     "start_time": "2020-11-23T13:32:49.785096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.315262,
     "end_time": "2020-11-23T13:32:50.132792",
     "exception": false,
     "start_time": "2020-11-23T13:32:49.817530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021311,
     "end_time": "2020-11-23T13:32:50.174973",
     "exception": false,
     "start_time": "2020-11-23T13:32:50.153662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.064816,
     "end_time": "2020-11-23T13:32:50.261340",
     "exception": false,
     "start_time": "2020-11-23T13:32:50.196524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[0]\n",
    "    H = size[1]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "        \n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, \n",
    "                 transforms=None, \n",
    "                 output_label=True, \n",
    "                 one_hot_label=False,\n",
    "                 do_fmix=False, \n",
    "                 fmix_params={\n",
    "                     'alpha': 1., \n",
    "                     'decay_power': 3., \n",
    "                     'shape': (CFG['img_size'], CFG['img_size']),\n",
    "                     'max_soft': True, \n",
    "                     'reformulate': False\n",
    "                 },\n",
    "                 do_cutmix=False,\n",
    "                 cutmix_params={\n",
    "                     'alpha': 1,\n",
    "                 }\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.do_fmix = do_fmix\n",
    "        self.fmix_params = fmix_params\n",
    "        self.do_cutmix = do_cutmix\n",
    "        self.cutmix_params = cutmix_params\n",
    "        \n",
    "        self.output_label = output_label\n",
    "        self.one_hot_label = one_hot_label\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "            #print(self.labels)\n",
    "            \n",
    "            if one_hot_label is True:\n",
    "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
    "                #print(self.labels)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "          \n",
    "        img  = get_img(\"{}\".format(self.df.loc[index]['path']))\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            with torch.no_grad():\n",
    "                #lam, mask = sample_mask(**self.fmix_params)\n",
    "                \n",
    "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n",
    "                \n",
    "                # Make mask, get mean / std\n",
    "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
    "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
    "    \n",
    "                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                fmix_img  = get_img(\"{}\".format(self.df.iloc[fmix_ix]['path']))\n",
    "\n",
    "                if self.transforms:\n",
    "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
    "\n",
    "                mask_torch = torch.from_numpy(mask)\n",
    "                \n",
    "                # mix image\n",
    "                img = mask_torch*img+(1.-mask_torch)*fmix_img\n",
    "\n",
    "                #print(mask.shape)\n",
    "\n",
    "                #assert self.output_label==True and self.one_hot_label==True\n",
    "\n",
    "                # mix target\n",
    "                rate = mask.sum()/CFG['img_size']/CFG['img_size']\n",
    "                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n",
    "                #print(target, mask, img)\n",
    "                #assert False\n",
    "        \n",
    "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            #print(img.sum(), img.shape)\n",
    "            with torch.no_grad():\n",
    "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                cmix_img  = get_img(\"{}\".format(self.df.iloc[cmix_ix]['path']))\n",
    "                if self.transforms:\n",
    "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
    "                    \n",
    "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox((CFG['img_size'], CFG['img_size']), lam)\n",
    "\n",
    "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (CFG['img_size'] * CFG['img_size']))\n",
    "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
    "                \n",
    "            #print('-', img.sum())\n",
    "            #print(target)\n",
    "            #assert False\n",
    "                            \n",
    "        # do label smoothing\n",
    "        #print(type(img), type(target))\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02183,
     "end_time": "2020-11-23T13:32:50.304795",
     "exception": false,
     "start_time": "2020-11-23T13:32:50.282965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Train\\Validation Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.590042,
     "end_time": "2020-11-23T13:32:50.916225",
     "exception": false,
     "start_time": "2020-11-23T13:32:50.326183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize, Rotate, \n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# def get_train_transforms():\n",
    "#     return Compose([\n",
    "#             RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "#             Transpose(p=0.5),\n",
    "#             Rotate(limit=(-90,90)),\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             VerticalFlip(p=0.5),\n",
    "# #             ShiftScaleRotate(p=0.5),\n",
    "# #             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "# #             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "# #             CoarseDropout(p=0.5),\n",
    "# #             Cutout(p=0.5),\n",
    "#             ToTensorV2(p=1.0),\n",
    "#         ], p=1.)\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "#             Rotate(limit=(-90,90)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_transform(size):\n",
    "#     return T.Compose([\n",
    "#         T.RandomApply([T.RandomAffine(45, shear=15)], 0.8),\n",
    "#         RandomResizedCropV2(size, scale=(0.6, 1.0), ratio=(3/5, 5/3)),\n",
    "#         T.RandomHorizontalFlip(),\n",
    "#         T.RandomVerticalFlip(),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         RandomErasing(probability=0.3, sh=0.3),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024452,
     "end_time": "2020-11-23T13:32:50.962106",
     "exception": false,
     "start_time": "2020-11-23T13:32:50.937654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.033239,
     "end_time": "2020-11-23T13:32:51.017593",
     "exception": false,
     "start_time": "2020-11-23T13:32:50.984354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self, arch='resnext50_32x4d_ssl', n=6, pre=True):\n",
    "#         super().__init__()\n",
    "#         m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', arch)\n",
    "#         self.enc = nn.Sequential(*list(m.children())[:-2])       \n",
    "#         nc = list(m.children())[-1].in_features\n",
    "#         self.head = nn.Sequential(AdaptiveConcatPool2d(),Flatten(),nn.Linear(2*nc,512),\n",
    "#                             Mish(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,n))\n",
    "        \n",
    "#     def forward(self, *x):\n",
    "#         shape = x[0].shape\n",
    "#         n = len(x)\n",
    "#         x = torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n",
    "#         #x: bs*N x 3 x 128 x 128\n",
    "#         x = self.enc(x)\n",
    "#         #x: bs*N x C x 4 x 4\n",
    "#         shape = x.shape\n",
    "#         #concatenate the output for tiles into a single map\n",
    "#         x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n",
    "#           .view(-1,shape[1],shape[2]*n,shape[3])\n",
    "#         #x: bs x C x N*4 x 4\n",
    "#         x = self.head(x)\n",
    "#         #x: bs x n\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class CassvaImgClassifier(nn.Module):\n",
    "#     def __init__(self, model_arch, n_class, pretrained=False):\n",
    "#         super().__init__()\n",
    "#         self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "#         n_features = self.model.classifier.in_features\n",
    "#         self.enc = nn.Sequential(*list(self.model.children())[:-2])\n",
    "# #         self.model.classifier = nn.Linear(n_features, n_class)\n",
    "#         self.head = nn.Sequential(AdaptiveConcatPool2d(),\n",
    "#                                               Flatten(),\n",
    "#                                               nn.Linear(2*n_features,512),\n",
    "#                                               Mish(),\n",
    "#                                               nn.BatchNorm1d(512), \n",
    "#                                               nn.Dropout(0.5),\n",
    "#                                               nn.Linear(512,n_class))\n",
    "            \n",
    "#         '''\n",
    "#         self.model.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             #nn.Linear(n_features, hidden_size,bias=True), nn.ELU(),\n",
    "#             nn.Linear(n_features, n_class, bias=True)\n",
    "#         )\n",
    "#         '''\n",
    "#     def forward(self, x):\n",
    "    \n",
    "#         shape = x.shape\n",
    "#         n = 1\n",
    "#         x = x.view(-1,shape[1],shape[2],shape[3])\n",
    "#         #x: bs*N x 3 x 128 x 128\n",
    "#         x = self.enc(x)\n",
    "#         #x: bs*N x C x 4 x 4\n",
    "#         shape = x.shape\n",
    "#         #concatenate the output for tiles into a single map\n",
    "#         x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous().view(-1,shape[1],shape[2]*n,shape[3])\n",
    "#         #x: bs x C x N*4 x 4\n",
    "#         x = self.head(x)\n",
    "#         #x: bs x n\n",
    "#         return x\n",
    "class AddGaussianNoise(nn.Module):\n",
    "    def __init__(self, mean=0., std=.1):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size(), device=CFG['device']) * self.std + self.mean\n",
    "        \n",
    "#     def __call__(self, tensor):\n",
    "#         return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)    \n",
    "\n",
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        num_features = self.model.classifier.in_features\n",
    "#         self.model.classifier = nn.Sequential(\n",
    "#             AddGaussianNoise(),\n",
    "#             nn.Linear(num_features, num_classes)\n",
    "#         )\n",
    "        self.model.classifier = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x    \n",
    "    \n",
    "# ====================================================\n",
    "# ResNext Model\n",
    "# ====================================================\n",
    "class CustomResNext(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        #='resnext50_32x4d',\n",
    "        num_features = self.model.fc.in_features\n",
    "#         self.model.fc = nn.Sequential(\n",
    "#             AddGaussianNoise(),\n",
    "#             nn.Linear(num_features, num_classes)\n",
    "#         )\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        ### vit\n",
    "        num_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(num_features, num_classes)\n",
    "#         self.model.head = nn.Sequential(\n",
    "#             AddGaussianNoise(),\n",
    "#             nn.Linear(num_features, num_classes)\n",
    "#         )\n",
    "        '''\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            #nn.Linear(num_features, hidden_size,bias=True), nn.ELU(),\n",
    "            nn.Linear(num_features, num_classes, bias=True)\n",
    "        )\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CustomViT\n",
    "# CFG = {\n",
    "#     'fold_num': 5,\n",
    "#     'seed': 719,\n",
    "#     'model_arch': 'vit_base_patch16_384',#'tf_efficientnet_b4_ns',\n",
    "#     'img_size': 384,\n",
    "#     'epochs': 10,\n",
    "#     'train_bs': 4,#16,\n",
    "#     'valid_bs': 4,#32,\n",
    "#     'T_0': 10,\n",
    "#     'lr': 1e-4,\n",
    "#     'min_lr': 1e-6,\n",
    "#     'weight_decay':1e-6,\n",
    "#     'num_workers': 0, #4\n",
    "#     'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "#     'verbose_step': 1,\n",
    "#     'device': 'cuda:0',\n",
    "#     'freeze_bn_epochs':5,\n",
    "# }\n",
    "\n",
    "# #CassvaImgClassifier\n",
    "# CFG = {\n",
    "#     'fold_num': 5,\n",
    "#     'seed': 719,\n",
    "#     'model_arch': 'vit_base_patch16_384',#'tf_efficientnet_b4_ns',\n",
    "#     'img_size': 512,\n",
    "#     'epochs': 10,\n",
    "#     'train_bs': 4,#16,\n",
    "#     'valid_bs': 4,#32,\n",
    "#     'T_0': 10,\n",
    "#     'lr': 1e-4,\n",
    "#     'min_lr': 1e-6,\n",
    "#     'weight_decay':1e-6,\n",
    "#     'num_workers': 0, #4\n",
    "#     'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "#     'verbose_step': 1,\n",
    "#     'device': 'cuda:0',\n",
    "#     'freeze_bn_epochs':5,\n",
    "# }\n",
    "\n",
    "# #CustomResNext\n",
    "# CFG = {\n",
    "#     'fold_num': 5,\n",
    "#     'seed': 719,\n",
    "#     'model_arch': 'resnext50_32x4d',#'tf_efficientnet_b4_ns',\n",
    "#     'img_size': 512,\n",
    "#     'epochs': 10,\n",
    "#     'train_bs': 4,#16,\n",
    "#     'valid_bs': 4,#32,\n",
    "#     'T_0': 10,\n",
    "#     'lr': 1e-4,\n",
    "#     'min_lr': 1e-6,\n",
    "#     'weight_decay':1e-6,\n",
    "#     'num_workers': 0, #4\n",
    "#     'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "#     'verbose_step': 1,\n",
    "#     'device': 'cuda:0',\n",
    "#     'freeze_bn_epochs':5,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021054,
     "end_time": "2020-11-23T13:32:51.059722",
     "exception": false,
     "start_time": "2020-11-23T13:32:51.038668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.061685,
     "end_time": "2020-11-23T13:32:51.144150",
     "exception": false,
     "start_time": "2020-11-23T13:32:51.082465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, oof_rate=0):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    if oof_rate>0:\n",
    "        train_oof = train_.sample(int(train_.shape[0]*oof_rate))\n",
    "        train_oof['label'] = train_oof['oof_label']\n",
    "        train_ = pd.concat([train_[~train_['image_id'].isin(train_oof.image_id.unique().tolist())], train_oof], axis=0)\n",
    "        train_ = train_.sample(train_.shape[0]).reset_index(drop=True)\n",
    "    \n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "        \n",
    "    train_ds = CassavaDataset(train_, transforms=get_train_transforms(), output_label=True, one_hot_label=False, do_fmix=False, do_cutmix=True)\n",
    "    valid_ds = CassavaDataset(valid_, transforms=get_valid_transforms(), output_label=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=True,        \n",
    "        num_workers=CFG['num_workers'],\n",
    "        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
    "    model.train()\n",
    "\n",
    "    t = time.time()\n",
    "    running_loss = None\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "\n",
    "        #print(image_labels.shape, exam_label.shape)\n",
    "        with autocast():\n",
    "            image_preds = model(imgs)   #output = model(input)\n",
    "            #print(image_preds.shape, exam_pred.shape)\n",
    "\n",
    "            loss = loss_fn(image_preds, image_labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01\n",
    "\n",
    "            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                if scheduler is not None and schd_batch_update:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                \n",
    "                pbar.set_description(description)\n",
    "                \n",
    "    if scheduler is not None and not schd_batch_update:\n",
    "        scheduler.step()\n",
    "        \n",
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
    "    model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        #print(image_preds.shape, exam_pred.shape)\n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, image_labels)\n",
    "        \n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]  \n",
    "\n",
    "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "            pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    validated_accuracy = (image_preds_all==image_targets_all).mean()\n",
    "    print('validation multi-class accuracy = {:.4f}'.format(validated_accuracy))\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        if schd_loss_update:\n",
    "            scheduler.step(loss_sum/sample_num)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    return validated_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.034873,
     "end_time": "2020-11-23T13:32:51.200704",
     "exception": false,
     "start_time": "2020-11-23T13:32:51.165831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
    "class MyCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        lsm = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            lsm = lsm * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = -(targets * lsm).sum(-1)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "    \n",
    "# version 1: use torch.autograd\n",
    "class TaylorSoftmax(nn.Module):\n",
    "    '''\n",
    "    This is the autograd version\n",
    "    '''\n",
    "    def __init__(self, dim=1, n=2):\n",
    "        super(TaylorSoftmax, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        usage similar to nn.Softmax:\n",
    "            >>> mod = TaylorSoftmax(dim=1, n=4)\n",
    "            >>> inten = torch.randn(1, 32, 64, 64)\n",
    "            >>> out = mod(inten)\n",
    "        '''\n",
    "        fn = torch.ones_like(x)\n",
    "        denor = 1.\n",
    "        for i in range(1, self.n+1):\n",
    "            denor *= i\n",
    "            fn = fn + x.pow(i) / denor\n",
    "        out = fn / fn.sum(dim=self.dim, keepdims=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "##\n",
    "# version 1: use torch.autograd\n",
    "class TaylorCrossEntropyLoss(nn.Module):\n",
    "    '''\n",
    "    This is the autograd version\n",
    "    '''\n",
    "    def __init__(self, n=2, ignore_index=-1, reduction='mean'):\n",
    "        super(TaylorCrossEntropyLoss, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        '''\n",
    "        usage similar to nn.CrossEntropyLoss:\n",
    "            >>> crit = TaylorCrossEntropyLoss(n=4)\n",
    "            >>> inten = torch.randn(1, 10, 64, 64)\n",
    "            >>> label = torch.randint(0, 10, (1, 64, 64))\n",
    "            >>> out = crit(inten, label)\n",
    "        '''\n",
    "        log_probs = self.taylor_softmax(logits).log()\n",
    "        loss = F.nll_loss(log_probs, labels, reduction=self.reduction,\n",
    "                ignore_index=self.ignore_index)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u'.\"\"\"\n",
    "    if t==1.0:\n",
    "        return u.log()\n",
    "    else:\n",
    "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
    "    if t==1:\n",
    "        return u.exp()\n",
    "    else:\n",
    "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = torch.sum(\n",
    "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "        normalized_activations = normalized_activations_step_0 * \\\n",
    "                logt_partition.pow(1.0-t)\n",
    "\n",
    "    logt_partition = torch.sum(\n",
    "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "    return normalization_constants\n",
    "\n",
    "def compute_normalization_binary_search(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (< 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations = activations - mu\n",
    "\n",
    "    effective_dim = \\\n",
    "        torch.sum(\n",
    "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
    "            dim=-1, keepdim=True).to(activations.dtype)\n",
    "\n",
    "    shape_partition = activations.shape[:-1] + (1,)\n",
    "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
    "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = (upper + lower)/2.0\n",
    "        sum_probs = torch.sum(\n",
    "                exp_t(normalized_activations - logt_partition, t),\n",
    "                dim=-1, keepdim=True)\n",
    "        update = (sum_probs < 1.0).to(activations.dtype)\n",
    "        lower = torch.reshape(\n",
    "                lower * update + (1.0-update) * logt_partition,\n",
    "                shape_partition)\n",
    "        upper = torch.reshape(\n",
    "                upper * (1.0 - update) + update * logt_partition,\n",
    "                shape_partition)\n",
    "\n",
    "    logt_partition = (upper + lower)/2.0\n",
    "    return logt_partition + mu\n",
    "\n",
    "class ComputeNormalization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, activations, t, num_iters):\n",
    "        if t < 1.0:\n",
    "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
    "        else:\n",
    "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "        ctx.save_for_backward(activations, normalization_constants)\n",
    "        ctx.t=t\n",
    "        return normalization_constants\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        activations, normalization_constants = ctx.saved_tensors\n",
    "        t = ctx.t\n",
    "        normalized_activations = activations - normalization_constants \n",
    "        probabilities = exp_t(normalized_activations, t)\n",
    "        escorts = probabilities.pow(t)\n",
    "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
    "        grad_input = escorts * grad_output\n",
    "        \n",
    "        return grad_input, None, None\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example. \n",
    "    Backward pass is implemented.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    return ComputeNormalization.apply(activations, t, num_iters)\n",
    "\n",
    "def tempered_sigmoid(activations, t, num_iters = 5):\n",
    "    \"\"\"Tempered sigmoid function.\n",
    "    Args:\n",
    "      activations: Activations for the positive class for binary classification.\n",
    "      t: Temperature tensor > 0.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
    "    return internal_probabilities[..., 0]\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature > 1.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    if t == 1.0:\n",
    "        return activations.softmax(dim=-1)\n",
    "\n",
    "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "\n",
    "class BiTemperedLogisticLoss(nn.Module):\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      labels: A tensor with shape and dtype as activations (onehot), \n",
    "        or a long tensor of one dimension less than activations (pytorch standard)\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
    "      num_iters: Number of iterations to run the method. Default 5.\n",
    "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
    "        ``'none'``: No reduction is applied, return shape is shape of\n",
    "        activations without the last dimension.\n",
    "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
    "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            t1=.2,\n",
    "            t2=1.2,\n",
    "            label_smoothing=0.1,\n",
    "            num_iters=5,\n",
    "            reduction = 'mean'):\n",
    "        super(BiTemperedLogisticLoss, self).__init__()\n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.num_iters = num_iters\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, activations, labels):\n",
    "\n",
    "        if len(labels.shape)<len(activations.shape): #not one-hot\n",
    "            labels_onehot = torch.zeros_like(activations)\n",
    "            labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "        else:\n",
    "            labels_onehot = labels\n",
    "\n",
    "        if self.label_smoothing > 0:\n",
    "            num_classes = labels_onehot.shape[-1]\n",
    "            labels_onehot = ( 1 - self.label_smoothing * num_classes / (num_classes - 1) ) \\\n",
    "                    * labels_onehot + \\\n",
    "                    self.label_smoothing / (num_classes - 1)\n",
    "\n",
    "        probabilities = tempered_softmax(activations, self.t2, self.num_iters)\n",
    "\n",
    "        loss_values = labels_onehot * log_t(labels_onehot + 1e-10, self.t1) \\\n",
    "                - labels_onehot * log_t(probabilities, self.t1) \\\n",
    "                - labels_onehot.pow(2.0 - self.t1) / (2.0 - self.t1) \\\n",
    "                + probabilities.pow(2.0 - self.t1) / (2.0 - self.t1)\n",
    "        loss_values = loss_values.sum(dim = -1) #sum over classes\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return loss_values\n",
    "        if self.reduction == 'sum':\n",
    "            return loss_values.sum()\n",
    "        if self.reduction == 'mean':\n",
    "            return loss_values.mean()\n",
    "        \n",
    "## t1 0.8 t2 1.2 / t1 0.2 t2 1.2 / t1 0.6 t2 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_with_data2019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cassava-leaf-disease-classification    21397\n",
       "cassava-disease                         5628\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = 'baseline_pytorch_efb4_v13'\n",
    "oof = pd.DataFrame()\n",
    "for file in os.listdir(path_):\n",
    "    if 'pkl' in file:\n",
    "        oof_i = pd.read_pickle(f'{path_}/{file}')\n",
    "        oof = pd.concat([oof, oof_i], axis=0)\n",
    "oof = oof.reset_index(drop=True)\n",
    "# oof['oof_label'] = oof[['label','soft_label_1','soft_label_2','soft_label_3','soft_label_4','soft_label_5']].apply(lambda x: x[int(1+x[0])], axis=1)\n",
    "oof['oof_label'] = oof[['soft_label_1','soft_label_2','soft_label_3','soft_label_4','soft_label_5']].apply(lambda x: np.argmax(list(x)), axis=1)\n",
    "train = pd.merge(train, oof[['image_id', 'oof_label']], how='left', on ='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>dataset</th>\n",
       "      <th>oof_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000015157.jpg</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>cassava-leaf-disease-classification</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000201771.jpg</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>cassava-leaf-disease-classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/100042118.jpg</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>cassava-leaf-disease-classification</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000723321.jpg</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>cassava-leaf-disease-classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>../input/cassava-leaf-disease-classification/train_images/1000812911.jpg</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>cassava-leaf-disease-classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        image_id  label  \\\n",
       "0      0  1000015157.jpg      0   \n",
       "1      1  1000201771.jpg      3   \n",
       "2      2   100042118.jpg      1   \n",
       "3      3  1000723321.jpg      1   \n",
       "4      4  1000812911.jpg      3   \n",
       "\n",
       "                                                                       path  \\\n",
       "0  ../input/cassava-leaf-disease-classification/train_images/1000015157.jpg   \n",
       "1  ../input/cassava-leaf-disease-classification/train_images/1000201771.jpg   \n",
       "2   ../input/cassava-leaf-disease-classification/train_images/100042118.jpg   \n",
       "3  ../input/cassava-leaf-disease-classification/train_images/1000723321.jpg   \n",
       "4  ../input/cassava-leaf-disease-classification/train_images/1000812911.jpg   \n",
       "\n",
       "   width  height                              dataset  oof_label  \n",
       "0    600     800  cassava-leaf-disease-classification          2  \n",
       "1    600     800  cassava-leaf-disease-classification          3  \n",
       "2    600     800  cassava-leaf-disease-classification          4  \n",
       "3    600     800  cassava-leaf-disease-classification          1  \n",
       "4    600     800  cassava-leaf-disease-classification          3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020806,
     "end_time": "2020-11-23T13:32:51.243006",
     "exception": false,
     "start_time": "2020-11-23T13:32:51.222200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 8602.016415,
     "end_time": "2020-11-23T15:56:13.280909",
     "exception": false,
     "start_time": "2020-11-23T13:32:51.264494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.0345: 100%|██████████| 1352/1352 [11:54<00:00,  1.89it/s]\n",
      "epoch 0 loss: 0.5169: 100%|██████████| 169/169 [01:16<00:00,  2.22it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 1.0145: 100%|██████████| 1352/1352 [11:57<00:00,  1.88it/s]\n",
      "epoch 1 loss: 0.5031: 100%|██████████| 169/169 [01:15<00:00,  2.24it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.9725: 100%|██████████| 1352/1352 [11:48<00:00,  1.91it/s]\n",
      "epoch 2 loss: 0.4795: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.9640: 100%|██████████| 1352/1352 [11:53<00:00,  1.90it/s]\n",
      "epoch 3 loss: 0.4651: 100%|██████████| 169/169 [01:17<00:00,  2.18it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 loss: 0.9433: 100%|██████████| 1352/1352 [11:49<00:00,  1.90it/s]\n",
      "epoch 4 loss: 0.4517: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.9379: 100%|██████████| 1352/1352 [11:58<00:00,  1.88it/s]\n",
      "epoch 5 loss: 0.4526: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.9201: 100%|██████████| 1352/1352 [12:00<00:00,  1.88it/s]\n",
      "epoch 7 loss: 0.8990: 100%|██████████| 1352/1352 [11:59<00:00,  1.88it/s]\n",
      "epoch 7 loss: 0.4441: 100%|██████████| 169/169 [01:13<00:00,  2.31it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.8959: 100%|██████████| 1352/1352 [11:33<00:00,  1.95it/s]\n",
      "epoch 8 loss: 0.4430: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 loss: 0.9071: 100%|██████████| 1352/1352 [11:29<00:00,  1.96it/s]\n",
      "epoch 9 loss: 0.4317: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8931\n",
      "Training with 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.0369: 100%|██████████| 1352/1352 [11:31<00:00,  1.96it/s]\n",
      "epoch 0 loss: 0.5050: 100%|██████████| 169/169 [01:22<00:00,  2.04it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 1.0181: 100%|██████████| 1352/1352 [11:40<00:00,  1.93it/s]\n",
      "epoch 1 loss: 0.4850: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.9834: 100%|██████████| 1352/1352 [11:53<00:00,  1.89it/s]\n",
      "epoch 2 loss: 0.4580: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.9538: 100%|██████████| 1352/1352 [11:45<00:00,  1.92it/s]\n",
      "epoch 3 loss: 0.4771: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 loss: 0.9537: 100%|██████████| 1352/1352 [11:49<00:00,  1.91it/s]\n",
      "epoch 4 loss: 0.4647: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.9209: 100%|██████████| 1352/1352 [11:50<00:00,  1.90it/s]\n",
      "epoch 5 loss: 0.4409: 100%|██████████| 169/169 [01:16<00:00,  2.22it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.9150: 100%|██████████| 1352/1352 [11:31<00:00,  1.95it/s]\n",
      "epoch 6 loss: 0.4445: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 loss: 0.9184: 100%|██████████| 1352/1352 [11:31<00:00,  1.95it/s]\n",
      "epoch 7 loss: 0.4438: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.9117: 100%|██████████| 1352/1352 [11:32<00:00,  1.95it/s]\n",
      "epoch 9 loss: 0.9025: 100%|██████████| 1352/1352 [11:34<00:00,  1.95it/s]\n",
      "epoch 9 loss: 0.4495: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8873\n",
      "Training with 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.0459: 100%|██████████| 1352/1352 [11:32<00:00,  1.95it/s]\n",
      "epoch 0 loss: 0.4996: 100%|██████████| 169/169 [01:15<00:00,  2.24it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.9909: 100%|██████████| 1352/1352 [11:36<00:00,  1.94it/s]\n",
      "epoch 1 loss: 0.4847: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.9838: 100%|██████████| 1352/1352 [12:00<00:00,  1.88it/s]\n",
      "epoch 2 loss: 0.4663: 100%|██████████| 169/169 [01:13<00:00,  2.28it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.9605: 100%|██████████| 1352/1352 [11:42<00:00,  1.92it/s]\n",
      "epoch 4 loss: 0.9375: 100%|██████████| 1352/1352 [11:45<00:00,  1.92it/s]\n",
      "epoch 4 loss: 0.4385: 100%|██████████| 169/169 [01:13<00:00,  2.28it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.9250: 100%|██████████| 1352/1352 [11:50<00:00,  1.90it/s]\n",
      "epoch 5 loss: 0.4312: 100%|██████████| 169/169 [01:15<00:00,  2.24it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.9292: 100%|██████████| 1352/1352 [11:50<00:00,  1.90it/s]\n",
      "epoch 6 loss: 0.4403: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 loss: 0.9066: 100%|██████████| 1352/1352 [11:48<00:00,  1.91it/s]\n",
      "epoch 7 loss: 0.4313: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.9124: 100%|██████████| 1352/1352 [11:48<00:00,  1.91it/s]\n",
      "epoch 8 loss: 0.4263: 100%|██████████| 169/169 [01:16<00:00,  2.20it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 loss: 0.9034: 100%|██████████| 1352/1352 [11:38<00:00,  1.94it/s]\n",
      "epoch 9 loss: 0.4172: 100%|██████████| 169/169 [01:13<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9032\n",
      "Training with 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.0283: 100%|██████████| 1352/1352 [11:32<00:00,  1.95it/s]\n",
      "epoch 0 loss: 0.5145: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.9823: 100%|██████████| 1352/1352 [11:35<00:00,  1.94it/s]\n",
      "epoch 1 loss: 0.4493: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.4817: 100%|██████████| 169/169 [01:13<00:00,  2.31it/s]s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.9484: 100%|██████████| 1352/1352 [11:33<00:00,  1.95it/s]\n",
      "epoch 3 loss: 0.4498: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 loss: 0.9402: 100%|██████████| 1352/1352 [11:35<00:00,  1.94it/s]\n",
      "epoch 4 loss: 0.4355: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.9374: 100%|██████████| 1352/1352 [11:39<00:00,  1.93it/s]\n",
      "epoch 5 loss: 0.4407: 100%|██████████| 169/169 [01:14<00:00,  2.28it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.9242: 100%|██████████| 1352/1352 [11:34<00:00,  1.95it/s]\n",
      "epoch 6 loss: 0.4341: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7 loss: 0.9068: 100%|██████████| 1352/1352 [11:35<00:00,  1.94it/s]\n",
      "epoch 7 loss: 0.4354: 100%|██████████| 169/169 [01:13<00:00,  2.29it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.9076: 100%|██████████| 1352/1352 [11:33<00:00,  1.95it/s]\n",
      "epoch 8 loss: 0.4378: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 loss: 0.8858: 100%|██████████| 1352/1352 [11:33<00:00,  1.95it/s]\n",
      "epoch 9 loss: 0.4316: 100%|██████████| 169/169 [01:13<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8969\n",
      "Training with 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.0515: 100%|██████████| 1352/1352 [11:28<00:00,  1.96it/s]\n",
      "epoch 0 loss: 0.5550: 100%|██████████| 169/169 [01:14<00:00,  2.28it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.4887: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.9788: 100%|██████████| 1352/1352 [11:31<00:00,  1.95it/s]\n",
      "epoch 2 loss: 0.4831: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.9513: 100%|██████████| 1352/1352 [11:32<00:00,  1.95it/s]\n",
      "epoch 3 loss: 0.4433: 100%|██████████| 169/169 [01:12<00:00,  2.34it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4 loss: 0.9357: 100%|██████████| 1352/1352 [11:29<00:00,  1.96it/s]\n",
      "epoch 4 loss: 0.4608: 100%|██████████| 169/169 [01:13<00:00,  2.31it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.9310: 100%|██████████| 1352/1352 [11:34<00:00,  1.95it/s]\n",
      "epoch 5 loss: 0.4474: 100%|██████████| 169/169 [01:12<00:00,  2.34it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.9325: 100%|██████████| 1352/1352 [11:31<00:00,  1.96it/s]\n",
      "epoch 7 loss: 0.9069: 100%|██████████| 1352/1352 [11:34<00:00,  1.95it/s]\n",
      "epoch 7 loss: 0.4520: 100%|██████████| 169/169 [01:13<00:00,  2.31it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.9051: 100%|██████████| 1352/1352 [11:32<00:00,  1.95it/s]\n",
      "epoch 8 loss: 0.4458: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n",
      "  0%|          | 0/1352 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9 loss: 0.9013: 100%|██████████| 1352/1352 [11:29<00:00,  1.96it/s]\n",
      "epoch 9 loss: 0.4341: 100%|██████████| 169/169 [01:12<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8918\n"
     ]
    }
   ],
   "source": [
    "################ freeze bn \n",
    "def freeze_batchnorm_stats(net):\n",
    "    try:\n",
    "        for m in net.modules():\n",
    "            if isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.LayerNorm):\n",
    "                m.eval()\n",
    "    except ValuError:\n",
    "        print('error with batchnorm2d or layernorm')\n",
    "        return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    valid= []\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "#         if fold in [0]:\n",
    "#             continue\n",
    "\n",
    "        print('Training with {} started'.format(fold))\n",
    "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, oof_rate=0.3)\n",
    "\n",
    "        device = torch.device(CFG['device'])\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         if torch.cuda.is_available():\n",
    "#             map_location=lambda storage, loc: storage.cuda()\n",
    "#         else:\n",
    "#             map_location='cpu'\n",
    "#         print('device ', device)\n",
    "        \n",
    "        model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique(), pretrained=True).to(device)\n",
    "        scaler = GradScaler()   \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
    "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=CFG['epochs']-1)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
    "#         scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=25, \n",
    "#                                                        max_lr=CFG['lr'], epochs=CFG['epochs'], steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        loss_tr = LabelSmoothingCrossEntropy(smoothing=0.2).to(device) #MyCrossEntropyLoss().to(device) BiTemperedLogisticLoss(t1=0.8, t2=1.2, label_smoothing=0.2)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        best_valid_accuracy = 0\n",
    "        for epoch in range(CFG['epochs']):\n",
    "#             if epoch < CFG['freeze_bn_epochs']:\n",
    "#                 freeze_batchnorm_stats(model)  \n",
    "            train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n",
    "            with torch.no_grad():\n",
    "                valid_accuracy = valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
    "                if valid_accuracy > best_valid_accuracy:\n",
    "                    best_valid_accuracy = valid_accuracy\n",
    "                    torch.save(model.state_dict(),'{}_fold_{}'.format(CFG['model_arch'], fold))\n",
    "#         torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n",
    "        del model, optimizer, train_loader, val_loader, scaler, scheduler\n",
    "        torch.cuda.empty_cache()\n",
    "        valid.append(best_valid_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8930619796484737,\n",
       " 0.8895467160037003,\n",
       " 0.9032377428307123,\n",
       " 0.8969472710453285,\n",
       " 0.8917668825161887]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8919518963922294]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ef4\n",
    "\n",
    "do cutmix valid　[0.8919518963922294]\n",
    "without cutmix [0.887881591119334]\n",
    "simple without cutmix [0.8902867715078631]\n",
    "add ratation without cutmix [0.8904717853839038]\n",
    "add ratation with cutmix [0.8930619796484737,0.8893617021276595,\n",
    " 0.8962072155411656,\n",
    " 0.8971322849213691,\n",
    " 0.8932469935245143]\n",
    "\n",
    "30% oof label with cutmix [0.8927 0.8921369102682701, 0.8993524514338576 0.8976873265494912, 0.8960222016651249]\n",
    "\n",
    "cutmix original_augumentation +2019data LabelSmoothingCrossEntropy CosineAnnealingWarmRestarts 0.8917668825161887\n",
    "cutmix original_augumentation +2019data LabelSmoothingCrossEntropy CosineAnnealingWarmRestarts 0.8938020351526365\n",
    "cutmix original_augumentation +2019data LabelSmoothingCrossEntropy OneCycleLR 0.8590 it is slow\n",
    "cutmix rotate original_augumentation +2019data LabelSmoothingCrossEntropy CosineAnnealingWarmRestarts 0.8917668825161887\n",
    "cutmix original_augumentation +2019data LabelSmoothingCrossEntropy CosineAnnealingWarmRestarts 0.8938020351526365\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 385.64it/s]\n",
      "100%|██████████| 27053/27053 [01:59<00:00, 225.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "# train['path'] = '../input/cassava-leaf-disease-classification/train_images/' + train['image_id']\n",
    "# path_ = '../input/cassava-disease/train'\n",
    "# data2019 = []\n",
    "# for label in tqdm(os.listdir(path_)):\n",
    "#     for file in os.listdir(f'{path_}/{label}'):\n",
    "#         data2019.append({'image_id':file, 'label':label, 'path':f'{path_}/{label}/{file}'})\n",
    "# data2019 = pd.DataFrame(data2019)\n",
    "# data2019['label'] = data2019['label'].replace({'cmd':3, 'cgm':2, 'cbsd':1, 'cbb':0, 'healthy':4})\n",
    "\n",
    "# train = pd.concat([train, data2019], axis= 0).reset_index(drop=True)\n",
    "# lst = []\n",
    "# for idx, row in tqdm(train.iterrows(), total = train.shape[0]):\n",
    "#     img = cv2.imread(row['path'])\n",
    "#     lst.append({'width':img.shape[0], 'height':img.shape[1]})\n",
    "# train = pd.concat([train, pd.DataFrame(lst)], axis=1)\n",
    "# train['dataset'] = train['path'].apply(lambda x: 'cassava-leaf-disease-classification' if 'cassava-leaf-disease-classification' in x else 'cassava-disease')\n",
    "# train = train[(train['width']>=500)&(train['height']>=500)].reset_index(drop=False)\n",
    "# train.to_pickle('train_with_data2019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference fold 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:18<00:00,  2.16it/s]\n",
      "100%|██████████| 169/169 [01:18<00:00,  2.16it/s]\n",
      "100%|██████████| 169/169 [01:18<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 validation loss = 0.42120\n",
      "fold 0 validation accuracy = 0.90823\n",
      "Inference fold 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:17<00:00,  2.17it/s]\n",
      "100%|██████████| 169/169 [01:19<00:00,  2.13it/s]\n",
      "100%|██████████| 169/169 [01:18<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 validation loss = 0.43108\n",
      "fold 1 validation accuracy = 0.90435\n",
      "Inference fold 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:18<00:00,  2.15it/s]\n",
      "100%|██████████| 169/169 [01:18<00:00,  2.15it/s]\n",
      "100%|██████████| 169/169 [01:18<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2 validation loss = 0.43397\n",
      "fold 2 validation accuracy = 0.90416\n",
      "Inference fold 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:16<00:00,  2.20it/s]\n",
      "100%|██████████| 169/169 [01:19<00:00,  2.13it/s]\n",
      "100%|██████████| 169/169 [01:16<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3 validation loss = 0.41563\n",
      "fold 3 validation accuracy = 0.91286\n",
      "Inference fold 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n",
      "100%|██████████| 169/169 [01:13<00:00,  2.31it/s]\n",
      "100%|██████████| 169/169 [01:12<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4 validation loss = 0.41230\n",
      "fold 4 validation accuracy = 0.92007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle('train_with_data2019.pkl')\n",
    "\n",
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'img_size': 456,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 32,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 0,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'weights': [1,1,1,1,1]\n",
    "}\n",
    "\n",
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all\n",
    "\n",
    "def get_inference_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(CFG['seed'])\n",
    "    his = []\n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "#         if fold>0:\n",
    "#             break\n",
    "\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "        valid_ds = CassavaDataset(valid_, transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        device = torch.device(CFG['device'])\n",
    "        model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        \n",
    "        val_preds = []\n",
    "        \n",
    "        #for epoch in range(CFG['epochs']-3):    \n",
    "        model.load_state_dict(torch.load('{}_fold_{}'.format(CFG['model_arch'], fold)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(CFG['tta']):\n",
    "                val_preds += [1/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n",
    "                \n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        \n",
    "        val_loss = log_loss(valid_.label.values, val_preds)\n",
    "        val_accuracy = (valid_.label.values==np.argmax(val_preds, axis=1)).mean()\n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, val_loss))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, val_accuracy))\n",
    "        his.append({'fold':fold, 'val_loss':val_loss, 'val_accuracy':val_accuracy})\n",
    "        \n",
    "        oof_ = pd.concat([valid_, pd.DataFrame(val_preds, columns=[f'soft_label_{i}' for i in range(1,6)])], axis=1)\n",
    "        oof_.to_pickle(f\"{CFG['model_arch']}_oof{fold}.pkl\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.90860,0.90879,0.90287,0.91637,0.92673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-037455c7868b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-81-037455c7868b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    v1 baseline accuracy = 0.8857\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "v1 baseline accuracy = 0.8857\n",
    "v2 v1=> change LabelSmoothingCrossEntropy accuracy = 0.8874\n",
    "v3 v2=> add freeze_batchnorm_stats accuracy = 0.8874 no change\n",
    "v4 v2=> remove some aug accuracy = 0.8937\n",
    "v5 v4=> use ViT accuracy = 0.8837\n",
    "v6 v4=> use resnext accuracy = 0.8734\n",
    "v7 v4=> only classify 0,1,2,4 cause 3 is too much accuracy = 0.85\n",
    "v8 v4=> remove noisy image by oof accuracy = 0.93\n",
    "v9 v4=> remove noisy by confidence \n",
    "v10 v4=> remove noisy by confidence \n",
    "v11 add noise layer, add 2019data taa3 0.912, taa8 0.918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-82-01d09d172650>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-82-01d09d172650>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    take 0.95 fold 0 validation accuracy = 0.91495\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "take 0.95 fold 0 validation accuracy = 0.91495\n",
    "confidence 0.95 fold 0 validation accuracy = 0.91776\n",
    "confidence 0.98 fold 0 validation accuracy = 0.91776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error +/- const * sqrt( (error * (1 - error)) / n)\n",
    "0.02 +/- 1.96 * sqrt( (0.02 * (1 - 0.02)) / 50)\n",
    "0.02 +/- 1.96 * sqrt(0.0196 / 50)\n",
    "0.02 +/- 1.96 * 0.0197\n",
    "0.02 +/- 0.0388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 8637.721674,
   "end_time": "2020-11-23T15:56:40.428882",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-23T13:32:42.707208",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
