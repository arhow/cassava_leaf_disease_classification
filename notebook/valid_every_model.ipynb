{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worthy-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_paths = [\n",
    "    '../input/pytorch-image-models/pytorch-image-models-master', #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
    "    '../input/image-fmix/FMix-master'\n",
    "]\n",
    "import sys; \n",
    "\n",
    "for pth in package_paths:\n",
    "    sys.path.append(pth)\n",
    "    \n",
    "# from fmix import sample_mask, make_low_freq_image, binarise_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "necessary-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangz\\anaconda3\\envs\\tf_gpu23-2\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.layers import AdaptiveConcatPool2d, Flatten, Mish\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charming-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        num_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x    \n",
    "\n",
    "class CustomResNext(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        #='resnext50_32x4d',\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        ### vit\n",
    "        num_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(num_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x  \n",
    "    \n",
    "class NFResNext(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        #='nf_resnet50',\n",
    "        num_features = self.model.head.fc.in_features\n",
    "        self.model.head.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "northern-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[0]\n",
    "    H = size[1]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "        \n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, \n",
    "                 img_size,\n",
    "                 transforms=None, \n",
    "                 output_label=True, \n",
    "                 one_hot_label=False,\n",
    "                 do_fmix=False, \n",
    "                 fmix_params={\n",
    "                     'alpha': 1., \n",
    "                     'decay_power': 3., \n",
    "                     'shape': (384, 384),\n",
    "                     'max_soft': True, \n",
    "                     'reformulate': False\n",
    "                 },\n",
    "                 do_cutmix=False,\n",
    "                 cutmix_params={\n",
    "                     'alpha': 1,\n",
    "                 }\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.do_fmix = do_fmix\n",
    "        self.fmix_params = fmix_params\n",
    "        self.do_cutmix = do_cutmix\n",
    "        self.cutmix_params = cutmix_params\n",
    "        \n",
    "        self.output_label = output_label\n",
    "        self.one_hot_label = one_hot_label\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "            #print(self.labels)\n",
    "            \n",
    "            if one_hot_label is True:\n",
    "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
    "                #print(self.labels)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "          \n",
    "        img  = get_img(\"{}\".format(self.df.loc[index]['path']))\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            with torch.no_grad():\n",
    "                #lam, mask = sample_mask(**self.fmix_params)\n",
    "                \n",
    "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n",
    "                \n",
    "                # Make mask, get mean / std\n",
    "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
    "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
    "    \n",
    "                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                fmix_img  = get_img(\"{}\".format(self.df.iloc[fmix_ix]['path']))\n",
    "\n",
    "                if self.transforms:\n",
    "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
    "\n",
    "                mask_torch = torch.from_numpy(mask)\n",
    "                \n",
    "                # mix image\n",
    "                img = mask_torch*img+(1.-mask_torch)*fmix_img\n",
    "\n",
    "                #print(mask.shape)\n",
    "\n",
    "                #assert self.output_label==True and self.one_hot_label==True\n",
    "\n",
    "                # mix target\n",
    "                rate = mask.sum()/self.img_size/self.img_size\n",
    "                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n",
    "                #print(target, mask, img)\n",
    "                #assert False\n",
    "        \n",
    "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            #print(img.sum(), img.shape)\n",
    "            with torch.no_grad():\n",
    "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                cmix_img  = get_img(\"{}\".format(self.df.iloc[cmix_ix]['path']))\n",
    "                if self.transforms:\n",
    "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
    "                    \n",
    "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox((self.img_size, self.img_size), lam)\n",
    "\n",
    "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (self.img_size * self.img_size))\n",
    "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
    "                \n",
    "            #print('-', img.sum())\n",
    "            #print(target)\n",
    "            #assert False\n",
    "                            \n",
    "        # do label smoothing\n",
    "        #print(type(img), type(target))\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ideal-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
    "    model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        #print(image_preds.shape, exam_pred.shape)\n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, image_labels)\n",
    "        \n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]  \n",
    "\n",
    "        if ((step + 1) % 1 == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "            pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    validated_accuracy = (image_preds_all==image_targets_all).mean()\n",
    "    print('validation multi-class accuracy = {:.4f}'.format(validated_accuracy))\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        if schd_loss_update:\n",
    "            scheduler.step(loss_sum/sample_num)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    return validated_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fiscal-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_folder_name(file):\n",
    "    if 'efb4' in file:\n",
    "        return CassvaImgClassifier, 'tf_efficientnet_b4_ns', 456\n",
    "    elif 'vit' in file:\n",
    "        return CustomViT, 'vit_base_patch16_384', 384\n",
    "    elif 'nfresnet' in file:\n",
    "        return object, 'nf_resnet50', 0\n",
    "    else:\n",
    "        return object,'',0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wooden-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prepared-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, GaussianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, RandomBrightness, RandomContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize, Rotate, \n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# def get_train_transforms():\n",
    "#     return Compose([\n",
    "#             RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "#             Transpose(p=0.5),\n",
    "#             Rotate(limit=(-90,90)),\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             VerticalFlip(p=0.5),\n",
    "# #             ShiftScaleRotate(p=0.5),\n",
    "# #             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "# #             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "# #             CoarseDropout(p=0.5),\n",
    "# #             Cutout(p=0.5),\n",
    "#             ToTensorV2(p=1.0),\n",
    "#         ], p=1.)\n",
    "\n",
    "def get_train_transforms(img_size):\n",
    "    return Compose([\n",
    "            RandomResizedCrop(img_size, img_size),\n",
    "            Transpose(p=0.5),\n",
    "#             Rotate(limit=(-90,90)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_train_transforms2(img_size):\n",
    "    \n",
    "    return Compose([\n",
    "        RandomResizedCrop(height=img_size, width=img_size),\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=1), RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5)]),\n",
    "#         OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.5,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        ShiftScaleRotate(\n",
    "            shift_limit=0.2,\n",
    "            scale_limit=0.2,\n",
    "            rotate_limit=20,\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            border_mode=cv2.BORDER_REFLECT_101,\n",
    "            p=1,\n",
    "        ),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "        CoarseDropout(p=0.5),\n",
    "        Cutout(p=0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms(img_size):\n",
    "    return Compose([\n",
    "            CenterCrop(img_size, img_size, p=1.),\n",
    "            Resize(img_size, img_size),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "current-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 384\n",
    "def prepare_dataloader(df, img_size, trn_idx, val_idx, train_transforms, valid_transforms, oof_rate=0,do_fmix=False, do_cutmix=False, ):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    if oof_rate>0:\n",
    "        train_oof = train_.sample(int(train_.shape[0]*oof_rate))\n",
    "        train_oof['label'] = train_oof['oof_label']\n",
    "        train_ = pd.concat([train_[~train_['image_id'].isin(train_oof.image_id.unique().tolist())], train_oof], axis=0)\n",
    "        train_ = train_.sample(train_.shape[0]).reset_index(drop=True)\n",
    "    \n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "        \n",
    "    train_ds = CassavaDataset(train_, img_size, transforms=train_transforms, output_label=True, one_hot_label=False, do_fmix=do_fmix, do_cutmix=do_cutmix)\n",
    "    valid_ds = CassavaDataset(valid_, img_size, transforms=valid_transforms, output_label=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=32,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=True,        \n",
    "        num_workers=0,\n",
    "        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hired-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_with_data2019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "correct-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_pytorch_efb4\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4110: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8969\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3929: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8979\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3761: 100%|██████████| 169/169 [01:14<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9038\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3949: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8994\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4056: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8931\n",
      "baseline_pytorch_efb4_2019_16batch_biloss_456\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3642: 100%|██████████| 169/169 [01:15<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8890\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3776: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8903\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3517: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8997\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3698: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8979\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3649: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8931\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3707: 100%|██████████| 169/169 [01:13<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8894\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3641: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8962\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3607: 100%|██████████| 169/169 [01:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8932\n",
      "baseline_pytorch_efb4_2019_16batch_biloss_456_cutmix_30oof\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3729: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8927\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3741: 100%|██████████| 169/169 [01:12<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8921\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3492: 100%|██████████| 169/169 [01:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8994\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3653: 100%|██████████| 169/169 [01:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8977\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3649: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8960\n",
      "baseline_pytorch_efb4_2019_16batch_sml2_456_cutmix\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4491: 100%|██████████| 169/169 [01:12<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8907\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4422: 100%|██████████| 169/169 [01:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8901\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4265: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9010\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4324: 100%|██████████| 169/169 [01:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8990\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4347: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8966\n",
      "baseline_pytorch_efb4_2019_16batch_sml2_456_cutmix_30oof\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4367: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8944\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4545: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8871\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4202: 100%|██████████| 169/169 [01:14<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9018\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4309: 100%|██████████| 169/169 [01:13<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8968\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4300: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8960\n",
      "baseline_pytorch_efb4_add_data2019_add_noise\n",
      "device  cuda\n",
      "exception baseline_pytorch_efb4_add_data2019_add_noise Error(s) in loading state_dict for CassvaImgClassifier:\n",
      "\tMissing key(s) in state_dict: \"model.classifier.weight\", \"model.classifier.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"model.classifier.1.weight\", \"model.classifier.1.bias\". \n",
      "baseline_pytorch_efb4_add_data2019_add_noise_16batch\n",
      "device  cuda\n",
      "exception baseline_pytorch_efb4_add_data2019_add_noise_16batch Error(s) in loading state_dict for CassvaImgClassifier:\n",
      "\tMissing key(s) in state_dict: \"model.classifier.weight\", \"model.classifier.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"model.classifier.1.weight\", \"model.classifier.1.bias\". \n",
      "baseline_pytorch_efb4_add_noise\n",
      "device  cuda\n",
      "exception baseline_pytorch_efb4_add_noise Error(s) in loading state_dict for CassvaImgClassifier:\n",
      "\tMissing key(s) in state_dict: \"model.classifier.weight\", \"model.classifier.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"model.classifier.1.weight\", \"model.classifier.1.bias\". \n",
      "baseline_pytorch_efb4_bitemplogisticloss\n",
      "device  cuda\n",
      "exception baseline_pytorch_efb4_bitemplogisticloss Error(s) in loading state_dict for CassvaImgClassifier:\n",
      "\tMissing key(s) in state_dict: \"model.classifier.weight\", \"model.classifier.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"model.classifier.1.weight\", \"model.classifier.1.bias\". \n",
      "baseline_pytorch_efb4_clean_train_v1\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4180: 100%|██████████| 169/169 [01:15<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8918\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3561: 100%|██████████| 169/169 [01:14<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9092\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4414: 100%|██████████| 169/169 [01:15<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8648\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.5105: 100%|██████████| 169/169 [01:15<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8507\n",
      "baseline_pytorch_efb4_clean_train_v2\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3948: 100%|██████████| 169/169 [01:15<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8927\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3941: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9001\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3654: 100%|██████████| 169/169 [01:15<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8999\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3809: 100%|██████████| 169/169 [01:15<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8945\n",
      "baseline_pytorch_efb4_clean_train_v3\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4170: 100%|██████████| 169/169 [01:14<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8934\n",
      "device  cuda\n",
      "exception baseline_pytorch_efb4_clean_train_v3 [Errno 2] No such file or directory: 'baseline_pytorch_efb4_clean_train_v3/tf_efficientnet_b4_ns_fold_1'\n",
      "baseline_pytorch_efb4_v13\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4470: 100%|██████████| 169/169 [01:15<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8940\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4314: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9021\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4425: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8982\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4317: 100%|██████████| 169/169 [01:15<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8931\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4509: 100%|██████████| 169/169 [01:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8895\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4172: 100%|██████████| 169/169 [01:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9032\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4341: 100%|██████████| 169/169 [01:16<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8918\n",
      "baseline_pytorch_nfresnet_v1\n",
      "baseline_pytorch_nfresnet_v2\n",
      "baseline_pytorch_nfresnet_v3\n",
      "baseline_pytorch_resnet_2019_16batch_biloss_456\n",
      "baseline_pytorch_resnet_2019_16batch_biloss_456_cutmix\n",
      "baseline_pytorch_resnext\n",
      "baseline_pytorch_vit\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3370: 100%|██████████| 169/169 [01:50<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9056\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3393: 100%|██████████| 169/169 [01:54<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9047\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3129: 100%|██████████| 169/169 [01:53<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9166\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3263: 100%|██████████| 169/169 [01:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9077\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3562: 100%|██████████| 169/169 [01:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8960\n",
      "baseline_pytorch_vit_2019_16batch_biloss_456\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3763: 100%|██████████| 169/169 [01:52<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8879\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3719: 100%|██████████| 169/169 [01:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8890\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3528: 100%|██████████| 169/169 [01:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8981\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3677: 100%|██████████| 169/169 [01:52<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8929\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3677: 100%|██████████| 169/169 [01:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8951\n",
      "baseline_pytorch_vit_v3\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3778: 100%|██████████| 169/169 [01:50<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8873\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3815: 100%|██████████| 169/169 [01:51<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8858\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3560: 100%|██████████| 169/169 [01:52<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8992\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3775: 100%|██████████| 169/169 [01:52<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8892\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3787: 100%|██████████| 169/169 [01:52<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8884\n",
      "baseline_pytorch_vit_v4\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3877: 100%|██████████| 169/169 [01:50<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8879\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3964: 100%|██████████| 169/169 [01:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8842\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3565: 100%|██████████| 169/169 [01:50<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8958\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3864: 100%|██████████| 169/169 [01:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8879\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3831: 100%|██████████| 169/169 [01:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8918\n",
      "baseline_pytorch_vit_v5\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.2890: 100%|██████████| 169/169 [01:50<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9227\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3158: 100%|██████████| 169/169 [01:50<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9140\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.2724: 100%|██████████| 169/169 [01:50<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9291\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3095: 100%|██████████| 169/169 [01:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9164\n",
      "device  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.2883: 100%|██████████| 169/169 [01:49<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = './'\n",
    "valid= []\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    if (os.path.isdir(f\"{path}{f}\")) and 'baseline' in f:\n",
    "        print(f)\n",
    "        MODEL, model_arch, img_size = create_model_from_folder_name(f)\n",
    "        if MODEL == object:\n",
    "            continue\n",
    "        \n",
    "        seed_everything(719)\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=719).split(np.arange(train.shape[0]), train.label.values)\n",
    "        try:\n",
    "        \n",
    "            for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "\n",
    "                train_transforms=get_train_transforms(img_size)\n",
    "                valid_transforms=get_valid_transforms(img_size)\n",
    "\n",
    "                train_loader, val_loader = prepare_dataloader(train, img_size, trn_idx, val_idx, oof_rate=0, train_transforms=train_transforms, valid_transforms=valid_transforms)\n",
    "\n",
    "        #         device = torch.device(CFG['device'])\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                if torch.cuda.is_available():\n",
    "                    map_location=lambda storage, loc: storage.cuda()\n",
    "                else:\n",
    "                    map_location='cpu'\n",
    "                print('device ', device)\n",
    "\n",
    "                model = MODEL(model_arch, train.label.nunique(), pretrained=True).to(device)    \n",
    "                model.load_state_dict(torch.load('{}/{}_fold_{}'.format(f, model_arch, fold)))\n",
    "                loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "                with torch.no_grad():\n",
    "                    valid_accuracy = valid_one_epoch(0, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
    "        #         torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n",
    "                del model, train_loader, val_loader\n",
    "                torch.cuda.empty_cache()\n",
    "                valid.append({'folder':f, 'fold':fold, 'val_accuracy':valid_accuracy})\n",
    "        except Exception as e:\n",
    "            print('exception', f, e.__str__())\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "worldwide-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "crucial-likelihood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder\n",
       "baseline_pytorch_efb4                                         0.898205\n",
       "baseline_pytorch_efb4_2019_16batch_biloss_456                 0.893617\n",
       "baseline_pytorch_efb4_2019_16batch_biloss_456_cutmix          0.893802\n",
       "baseline_pytorch_efb4_2019_16batch_biloss_456_cutmix_30oof    0.895578\n",
       "baseline_pytorch_efb4_2019_16batch_sml2_456_cutmix            0.895467\n",
       "baseline_pytorch_efb4_2019_16batch_sml2_456_cutmix_30oof      0.895208\n",
       "baseline_pytorch_efb4_clean_train_v1                          0.880185\n",
       "baseline_pytorch_efb4_clean_train_v2                          0.898612\n",
       "baseline_pytorch_efb4_clean_train_v3                          0.893432\n",
       "baseline_pytorch_efb4_v13                                     0.895763\n",
       "baseline_pytorch_efb4_v14                                     0.894912\n",
       "baseline_pytorch_vit                                          0.906124\n",
       "baseline_pytorch_vit_2019_16batch_biloss_456                  0.892581\n",
       "baseline_pytorch_vit_v3                                       0.889991\n",
       "baseline_pytorch_vit_v4                                       0.889510\n",
       "baseline_pytorch_vit_v5                                       0.921332\n",
       "Name: val_accuracy, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.groupby('folder')['val_accuracy'].agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-postcard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
